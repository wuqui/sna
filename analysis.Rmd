---
author: 'Quirin WÃ¼rschinger'
title: "Social networks of lexical innovation"
output: 
  html_notebook: 
    toc: yes
---

# Load code

```{r include=FALSE}
source('src/load-data.R')
source('src/postproc.R')
source('src/uses.R')
source('src/users.R')
source('src/sna.R')

library(corrr)
library(tidyr)
library(magrittr)
```


# Variables

```{r include=FALSE}
subsetting = 'time'
diff_start_method <- 'edges'
diff_start_limit <- 3

cases <- c(
  'upcycling',
  'alt-left', 
  'solopreneur',
  'hyperlocal'
  # 'ghosting', 
  # 'lituation', 
  )

```


# Load data

```{r include=FALSE}
if (exists('df_comp') == FALSE) {
  df_comp <- read_df_comp(f_path='out/df_comp.csv')
}
```


# Filter data

```{r include=FALSE}
df_comp %<>% 
  filter(
    SKIP != TRUE,
    SUBSETTING == subsetting,
    # established words
    !LEMMA %in% c('Anglo-Saxon', 'climate crisis', 'global heating', 'greenwashing', 'political correctness', 'refugee crisis ')
    # successful words: USES_TOT >= 10000
    # unsuccessful words: USES_TOT <= 10000
    # weird words: big dick energy
  )
```


## success

```{r}
df_success <- df_comp %>%
  select(LEMMA, SUBSET, USES, CENT_DEGREE) %>%
  filter(SUBSET != 'full') %>%
  group_by(LEMMA) %>%
  mutate(DIFF_USES = USES - lag(USES)) %>%
  mutate(DIFF_USES_TOT = sum(DIFF_USES, na.rm=TRUE)) %>%
  filter(SUBSET == 'four') %>%
  arrange(DIFF_USES_TOT)

successful_diffs <- df_success %>%
  filter(DIFF_USES_TOT > 0) %>%
  pull(LEMMA)

unsuccessful_diffs <- df_success %>%
  filter(DIFF_USES_TOT < 0) %>%
  pull(LEMMA)

successful_uses_min <- df_comp %>%
  filter(USES > 1000) %>%
  pull(LEMMA)
  
  
unsuccessful_uses_min <- df_comp %>%
  filter(USES < 1000) %>%
  pull(LEMMA)
```


# Single lemma

## Load data

```{r include=FALSE}
corpus <- '/Volumes/qjd/twint/'
lemma <- 'big dick energy'

tweets <- load_data(corpus, lemma)
tweets <- postproc(tweets)
tweets <- filter_tweets(tweets)
```


## Check tweets

```{r}
tweets %>%
  select(tweet, date) %>%
  # slice(., sample(1:n())) #random selection
  arrange(date)
```


## Check uses and edges

```{r}
df_comp %>%
  filter(LEMMA == 'hyperlocal') %>%
  select(LEMMA, USES_TOT, SUBSET, USES)
```


# Case studies

## Data overview

```{r include=FALSE}
df_comp %>%
  filter(
    LEMMA %in% cases,
    SUBSET == 'full'
    ) %>%
  select(LEMMA, USES=USES_TOT, SPEAKERS=USERS_TOT) %>%
  mutate(USES_PER_SPEAKER = USES / SPEAKERS) %>%
  arrange(desc(USES))
  # ggplot(data=., aes(x=USERS_TOT, y=USES_TOT)) +
  #   geom_text(aes(label=LEMMA)) +
  #   scale_y_continuous('frequency (log)', trans='log')
```


## Usage frequency

### Overall across cases

```{r}
df_comp %>%
  filter(
    LEMMA %in% cases,
    SUBSET == 'full'
    ) %>%
   arrange(desc(USES_TOT))
```


### Uses in first subset

```{r}
df_comp %>%
  filter(
    SUBSETTING == 'time',
    SUBSET == 'one'
    ) %>%
  select(LEMMA, USES_TOT, USES) %>%
  arrange(USES)
```



## Degree centralization

### Diachronic

```{r}
plt <- df_comp %>%
  select(LEMMA, SUBSETTING, SUBSET, CENT_DEGREE, CENT_EV, DENSITY) %>%
  filter(
    SUBSET != 'full',
    LEMMA %in% cases
    # LEMMA %in% c(cases, 'hyperlocal', 'blockchain', 'climate denial', 'man bun', 'upskill', 'deep learning')
    # overtourism: diff too big for case study scale; 
    # , 'broflake', 'climate crisis', 'incel', 'overtourism'
  ) %>%
  ggplot(., aes(x=SUBSET, y=CENT_DEGREE)) + # group=1
    geom_point(aes(group=LEMMA, color=LEMMA, shape=LEMMA)) +
    geom_line(aes(group=LEMMA, color=LEMMA, linetype=LEMMA)) +
    guides(group=TRUE) +
    ggtitle('Diffusion over time: changes in degree centralization') +
    scale_y_continuous('degree centrality') +
    scale_x_discrete('subset')

ggplotly(plt)
# ggsave('out/cases_cent_diac.pdf', width=6, height=4)
```


### Overall

```{r}
df_comp %>%
  filter(
    LEMMA %in% cases,
    SUBSET == 'full'
    ) %>%
  select(
    LEMMA, 
    EDGES,
    CENT_DEGREE, 
    CENT_EV
    ) %>%
  arrange((CENT_DEGREE))
```


# Full sample

## Usage intensity

```{r}
df_comp %>%
  filter(SUBSET == 'full') %>%
  arrange(desc(USES))
```


## Edges

```{r include=FALSE}
df_comp %>%
  dplyr::group_by(LEMMA) %>%
  filter(SUBSET == 'one') %>%
  select(LEMMA, SUBSET, USES, EDGES) %>%
  arrange(EDGES)
```


## Degree centrality

### Overall

#### List

```{r}
df_comp %>%
  select(LEMMA, SUBSET, USES_TOT, CENT_DEGREE, CENT_EV) %>%
  filter(
    SUBSET == 'full'
    # USES >= 2
    ) %>%
  arrange(
    (CENT_DEGREE)
    # desc(CENT_EV)
  )
```


#### Plot

```{r}
plt <- df_comp %>%
  select(LEMMA, SUBSET, USES, CENT_DEGREE) %>%
  filter(SUBSET == 'full') %>%
  arrange((CENT_DEGREE)) %>%
  ggplot(., aes(x=CENT_DEGREE, y=reorder(LEMMA, CENT_DEGREE))) +
    geom_point() +
    scale_y_discrete('lemmas') +
    scale_x_continuous(
      'degree centralization (log)',
      trans='log'
      )

plt

# ggsave('out/cent_sync_all.pdf', width=6, height=4)
```


### Over time

#### Across all lemmas

```{r}
df_comp %>%
  filter(
    SUBSET %in% c('one', 'two', 'three', 'four'),
    LEMMA %in% unsuccessful_diffs
    # USES_TOT >= 10000
    # USES > 1000
    ) %>%
  group_by(SUBSET) %>%
  summarize(
    DENS_AVG = mean(DENSITY),
    CENT_AVG = mean(CENT_DEGREE)
    ) %>%
  ggplot(., aes(x=SUBSET, y=DENS_AVG, group=1)) +
    geom_line() +
    geom_point() +
    scale_y_continuous('degree centralization') +
    scale_x_discrete('subsets')

# ggsave('out/full_cent_diac.pdf', width=6, height=4)
```


#### Biggest changes

```{r}
df_comp %>%
  select(LEMMA, SUBSET, CENT_DEGREE, EDGES, USES_TOT) %>%
  filter(
    SUBSET %in% c(
      'one', 
      'four'
      ),
    USES_TOT >= 10000
    ) %>%
  dplyr::group_by(LEMMA) %>%
  dplyr::mutate(CENT_DIFF = CENT_DEGREE - lag(CENT_DEGREE, default=CENT_DEGREE[1])) %>%
  drop_na() %>%
  select(-SUBSET) %>%
  rename(
    CENT_LAST = CENT_DEGREE,
    EDGES_LAST = EDGES
    ) %>%
  arrange((CENT_DIFF))
```


# Density

## overall

```{r}
df_comp %>%
  select(LEMMA, SUBSET, USES_TOT, CENT_DEGREE, DENSITY) %>%
  filter(SUBSET == 'four') %>%
  arrange(DENSITY)
```


## over time

### case studies

```{r}
df_comp %>%
  select(LEMMA, SUBSETTING, SUBSET, CENT_DEGREE, DENSITY) %>%
  filter(
    SUBSET != 'full',
    LEMMA %in% c(cases, 'twitterer', 'robo-signing', 'covfefe')
  ) %>%
  ggplot(., aes(x=SUBSET, y=DENSITY)) + # group=1
    geom_point(aes(group=LEMMA, color=LEMMA, shape=LEMMA)) +
    geom_line(aes(group=LEMMA, color=LEMMA, linetype=LEMMA)) +
    guides(group=TRUE) +
    ggtitle('Diffusion over time: changes in degree centralization') +
    scale_y_continuous('density') +
    scale_x_discrete('subset')

# ggsave('out/cases_cent_diac.pdf', width=6, height=4)
```


### full sample

```{r}
df_comp %>%
  filter(
    SUBSET %in% c('one', 'two', 'three', 'four'),
    LEMMA %in% unsuccessful_diffs,
    USES_TOT >= 10000
    ) %>%
  group_by(SUBSET) %>%
  summarize(
    DENS_AVG = mean(DENSITY)
    ) %>%
  ggplot(., aes(x=SUBSET, y=DENS_AVG, group=1)) +
    geom_line() +
    geom_point() +
    scale_y_continuous('density') +
    scale_x_discrete('subsets')
```


### biggest changes

```{r}
df_comp %>%
  select(LEMMA, SUBSET, CENT_DEGREE, DENSITY, EDGES, USES_TOT) %>%
  filter(
    SUBSET %in% c(
      'one', 
      'four'
      )
    # USES_TOT >= 10000
    ) %>%
  dplyr::group_by(LEMMA) %>%
  dplyr::mutate(DENS_DIFF = DENSITY - lag(DENSITY, default=DENSITY[1])) %>%
  drop_na() %>%
  select(-SUBSET) %>%
  rename(
    DENS_LAST = DENSITY,
    EDGES_LAST = EDGES
    ) %>%
  arrange((DENS_DIFF))
```


# Frequency vs. networks

## Frequency vs. centralization

### Plot

```{r}
plt <- df_comp %>%
  filter(
    SUBSET == 'four',
    # USES_TOT %in% (150000:500000)
    # LEMMA %in% c(cases)
    # !LEMMA %in% c('slut shaming', 'dashcam', 'shareable', 'cuckold', 'deep learning', 'hyperlocal')
    ) %>%
  select(LEMMA, CENT_DEGREE, USES_TOT, USES, EDGES) %>%
  ggplot(., aes(x=CENT_DEGREE, y=USES_TOT)) +
    geom_text(aes(label=LEMMA), hjust=-0.1, vjust=-0.1) + 
    # geom_point() +
    scale_y_continuous(
      'usage frequency (log)', 
      trans='log'
      ) +
    scale_x_continuous(
      'degree centralization'
      # trans='log'
      )

ggplotly(plt)
# ggsave('out/full_cent_freq_overall.pdf', width=6, height=4)
# ggsave('out/cases_cent_freq_overall.pdf', width=6, height=4)
```


### Biggest discrepancies

```{r}
df_comp %>%
  filter(
    SUBSETTING == 'time',
    SUBSET == 'four'
    ) %>%
  select(LEMMA, USES_TOT, CENT_DEGREE) %>%
  mutate(DISC = USES_TOT / CENT_DEGREE) %>%
  arrange(DISC)
```


### Correlation

```{r}
df_corr <- df_comp %>%
  filter(
    # SUBSET != 'full'
    # EDGES >= 100
    ) %>%
  select(-c(LEMMA, SUBSET, START, END, SKIP, STAMP))
  
cor.test(df_corr$USES, df_corr$CENT_DEGREE)
```


# Coefficient of variation

```{r}
df_comp %>%
  filter(
    SUBSET == 'full',
    USES_TOT >= 1000
    ) %>%
  select(LEMMA, USES_TOT, COEF_VAR) %>%
  arrange(desc(COEF_VAR))
```


# Processing status

## Lemma list

```{r}
df_comp %>%
  select(LEMMA, SUBSET, STAMP) %>%
  filter(SUBSET == 'four') %>%
  # mutate(STAMP = as_datetime(STAMP)) %>%
  arrange(desc(STAMP))
```


## Dataset statistics

```{r}
df_comp %>%
  filter(SUBSET == 'full') %>%
  select(LEMMA, SUBSET, USES_TOT, USERS_TOT) %>%
  dplyr::summarise(
    USES_ALL = sum(USES_TOT),
    USERS_ALL = sum(USERS_TOT)
    )
```