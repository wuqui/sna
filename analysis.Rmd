---
author: 'Quirin WÃ¼rschinger'
title: "Social networks of lexical innovation"
output: 
  html_notebook: 
    toc: yes
---

# Load code

```{r include=FALSE}
source('src/load-data.R')
source('src/postproc.R')
source('src/uses.R')
source('src/users.R')
source('src/sna.R')
source('src/analysis.R')

library(corrr)
library(tidyr)
library(magrittr)
library(forcats)
```


# Variables

```{r include=FALSE}
corpus <- '/Volumes/qjd/twint/'

subsetting = 'time'
diff_start_method <- 'edges'
diff_start_limit <- 3

cases <- c(
  # 'lituation'
  # 'ghosting',
  # 'upcycling',
  # 'birther'
  # 'circular economy',
  'upskill',
  'hyperlocal',
  'solopreneur',
  'alt-right',
  'alt-left',
  'poppygate'
  )

```


# Load data

```{r include=FALSE}
if (exists('df_comp') == FALSE) {
  df_comp <- read_df_comp(f_path='out/df_comp.csv')
}
```


# Filter data

```{r include=FALSE}
df_comp %<>% 
  filter(
    SKIP != TRUE,
    SUBSETTING == subsetting,
    # established words
    !LEMMA %in% c('Anglo-Saxon', 'climate crisis', 'global heating', 'greenwashing', 'political correctness', 'refugee crisis', 'artificial intelligence'),
    # successful words: USES_TOT >= 10000
    # unsuccessful words: USES_TOT <= 10000
    # weird words: big dick energy
    !LEMMA %in% c('cookprocessor')
  )

df_comp %>%
  View()
```


## success

```{r}
df_success <- df_comp %>%
  select(LEMMA, SUBSET, USES, CENT_DEGREE) %>%
  filter(SUBSET != 'full') %>%
  group_by(LEMMA) %>%
  mutate(DIFF_USES = USES - lag(USES)) %>%
  mutate(DIFF_USES_TOT = sum(DIFF_USES, na.rm=TRUE)) %>%
  filter(SUBSET == 'four') %>%
  arrange(DIFF_USES_TOT)

successful_diffs <- df_success %>%
  filter(DIFF_USES_TOT > 0) %>%
  pull(LEMMA)

unsuccessful_diffs <- df_success %>%
  filter(DIFF_USES_TOT < 0) %>%
  pull(LEMMA)

successful_uses_min <- df_comp %>%
  filter(USES > 1000) %>%
  pull(LEMMA)
  
  
unsuccessful_uses_min <- df_comp %>%
  filter(USES < 1000) %>%
  pull(LEMMA)
```


# Single lemma

## Load data

```{r include=FALSE}
lemma <- 'hyperlocal'
tweets <- load_data(corpus, lemma)
tweets <- postproc(tweets)
tweets <- filter_tweets(tweets)
```


## Check tweets

```{r}
tweets %>%
  select(tweet, date, replies_count, retweets_count, likes_count) %>%
  # slice(., sample(1:n())) #random selection
  arrange(desc(likes_count)) %>%
  View()
```


## Check uses and edges

```{r}
df_comp %>%
  filter(LEMMA == 'hyperlocal') %>%
  select(LEMMA, USES_TOT, SUBSET, USES)
```


# Case studies

## Data overview

```{r include=FALSE}
df_comp %>%
  filter(
    LEMMA %in% cases,
    SUBSET == 'full'
    ) %>%
  select(LEMMA, USES=USES_TOT, SPEAKERS=USERS_TOT) %>%
  mutate(USES_PER_SPEAKER = USES / SPEAKERS) %>%
  arrange(desc(USES))
  # ggplot(data=., aes(x=USERS_TOT, y=USES_TOT)) +
  #   geom_text(aes(label=LEMMA)) +
  #   scale_y_continuous('frequency (log)', trans='log')
```


## Usage frequency

### Overall across cases

```{r}
df_comp %>%
  filter(
    LEMMA %in% cases,
    SUBSET == 'full'
    ) %>%
   arrange(desc(USES_TOT))
```


### Uses in first subset

```{r}
df_comp %>%
  filter(
    SUBSETTING == 'time',
    SUBSET == 'one'
    ) %>%
  select(LEMMA, USES_TOT, USES) %>%
  arrange(USES)
```



## Degree centralization

### Diachronic

```{r}
cent_diac_incr <- c('robo-signing', 'circular economy', 'alt-right', 'Brexiteer')
cent_diac_const <- c('upskill')
cent_diac_decr <- c('blockchain', 'newsjacking', 'overtourism')


df_comp %>%
  select(LEMMA, SUBSETTING, SUBSET, CENT_DEGREE, CENT_EV, DENSITY) %>%
  filter(
    SUBSET != 'full',
    LEMMA %in% c(cases, 'alt-right'),
    # 'social justice warrior', 'baecation', 'tookkah', 'upskirting', 'helicopter parenting', 
    # 'glamping', 'lifehack', 'alt-right', 'lituation', 'baeless'
    !LEMMA %in% c('circular economy')
  ) %>%
  ggplot(., aes(x=SUBSET, y=CENT_DEGREE)) + # group=1
    geom_point(aes(group=LEMMA, color=LEMMA)) +
    geom_line(aes(group=LEMMA, color=LEMMA, linetype=LEMMA)) +
    guides(group=TRUE) +
    scale_y_continuous('degree centrality') +
    scale_x_discrete('subset') +
    scale_color_discrete('lemma') +
    scale_linetype_discrete(guide=FALSE)

# ggplotly(plt)
# ggsave('out/cases_cent_diac.pdf', width=6, height=4)
```


### Overall

```{r}
df_comp %>%
  filter(
    LEMMA %in% cases,
    SUBSET == 'full'
    ) %>%
  select(
    LEMMA, 
    EDGES,
    CENT_DEGREE, 
    CENT_EV
    ) %>%
  arrange((CENT_DEGREE))
```


# Full sample

# Frequency

## dataset statistics

```{r}
df_stats <- tibble(
  STAT = c('MIN', 'MAX', 'MEAN', 'MEDIAN'),
  VALUE = c(min(df_comp$USES_TOT), max(df_comp$USES_TOT), mean(df_comp$USES_TOT), median(df_comp$USES_TOT))
  )

exs_min <- df_comp %>% 
  filter(SUBSET == 'full') %>%
  arrange(USES_TOT) %>%
  select(LEMMA, USES_TOT) %>%
  top_n(-6, USES_TOT) %>%
  mutate(GROUP = 'min')
  
exs_max <- df_comp %>% 
  filter(SUBSET == 'full') %>%
  arrange(desc(USES_TOT)) %>%
  select(LEMMA, USES_TOT) %>%
  top_n(6, USES_TOT) %>%
  mutate(GROUP = 'max')

exs_median <- df_comp %>% 
  filter(SUBSET == 'full') %>%
  arrange(desc(USES_TOT)) %>%
  filter(USES_TOT <= df_stats %>% filter(STAT == 'MEDIAN') %>% pull(VALUE)) %>%
  select(LEMMA, USES_TOT) %>%
  top_n(6, USES_TOT) %>%
  mutate(GROUP = 'median')

exs_mean <- df_comp %>% 
  filter(SUBSET == 'full') %>%
  arrange(USES_TOT) %>%
  filter(USES_TOT >= df_stats %>% filter(STAT == 'MEAN') %>% pull(VALUE)) %>%
  select(LEMMA, USES_TOT) %>%
  top_n(-6, USES_TOT) %>%
  mutate(GROUP = 'mean')

exs_cases <- df_comp %>%
  filter(SUBSET == 'full') %>%
  arrange(desc(USES_TOT)) %>%
  filter(LEMMA %in% cases) %>%
  select(LEMMA, USES_TOT) %>%
  mutate(GROUP = 'case')

df_examples <- bind_rows(exs_min, exs_max, exs_median, exs_mean, exs_cases)

df_examples %<>% 
  mutate(GROUP = factor(GROUP, c('min', 'median', 'mean', 'max', 'case'))) %>%
  arrange(GROUP)

# write_csv(df_examples, 'out/df_examples.csv')
```



## list of totals

```{r}
df_comp %>%
  filter(SUBSET == 'full') %>%
  # filter(LEMMA %in% c(cases)) %>%
  arrange(desc(USES_TOT))
```


## cumulative

```{r}
# uses_lemmas <- NULL
lemmas_done <- uses_lemmas %>% distinct(LEMMA) %>% pull()

# for (lemma in c('alt-right')) {
#   if (!lemma %in% lemmas_done) {
#     tweets <- load_data(corpus, lemma)
#     tweets <- postproc(tweets)
#     tweets <- filter_tweets(tweets)
#     uses <- get_uses(tweets)
#     uses_month <- bin_uses(uses, 'month')
#     uses_month %<>%
#       mutate(LEMMA = lemma) %>%
#       mutate(USES_CUM = cumsum(USES))
#     uses_lemmas <- bind_rows(uses_lemmas, uses_month)
#   }
# }

uses_lemmas %>%
  mutate(LEMMA = fct_reorder(LEMMA, USES_CUM, max, .desc=TRUE)) %>%
  filter(LEMMA %in% c(cases)) %>%
  filter(LEMMA != 'alt-right') %>%
  ggplot(data=., aes(x=DATE, y=USES_CUM)) +
    geom_line(aes(color=LEMMA, linetype=LEMMA)) +
    scale_y_continuous('cumulative usage frequency') +
    scale_x_date('') +
    scale_color_discrete('lemma') +
    scale_linetype_discrete(guide=FALSE)

# ggsave('out/freq_cum_cases.pdf', width=6, height=4)
# write_csv(uses_lemmas, 'out/uses_lemmas.csv')
```


# Edges

```{r include=FALSE}
df_comp %>%
  dplyr::group_by(LEMMA) %>%
  filter(SUBSET == 'one') %>%
  select(LEMMA, SUBSET, USES, EDGES) %>%
  arrange(EDGES)
```


# Degree centrality

## Overall

### List

```{r}
df_comp %>%
  select(LEMMA, SUBSET, USES_TOT, CENT_DEGREE, CENT_EV) %>%
  filter(
    SUBSET == 'full'
    # USES >= 2
    ) %>%
  arrange(
    (CENT_DEGREE)
    # desc(CENT_EV)
  )
```


### Plot

```{r}
plt <- df_comp %>%
  select(LEMMA, SUBSET, USES, CENT_DEGREE) %>%
  filter(SUBSET == 'full') %>%
  arrange((CENT_DEGREE)) %>%
  ggplot(., aes(x=CENT_DEGREE, y=reorder(LEMMA, CENT_DEGREE))) +
    geom_point() +
    scale_y_discrete('lemmas') +
    scale_x_continuous(
      'degree centralization (log)',
      trans='log'
      )

plt

# ggsave('out/cent_sync_all.pdf', width=6, height=4)
```


### Over time

#### Across all lemmas

```{r}
df_comp %>%
  filter(
    SUBSET %in% c('one', 'two', 'three', 'four'),
    LEMMA %in% unsuccessful_diffs
    # USES_TOT >= 10000
    # USES > 1000
    ) %>%
  group_by(SUBSET) %>%
  summarize(
    DENS_AVG = mean(DENSITY),
    CENT_AVG = mean(CENT_DEGREE)
    ) %>%
  ggplot(., aes(x=SUBSET, y=DENS_AVG, group=1)) +
    geom_line() +
    geom_point() +
    scale_y_continuous('degree centralization') +
    scale_x_discrete('subsets')

# ggsave('out/full_cent_diac.pdf', width=6, height=4)
```


#### Biggest changes

```{r}
df_comp %>%
  select(LEMMA, SUBSET, CENT_DEGREE, EDGES, USES_TOT) %>%
  filter(
    SUBSET %in% c('one', 'four'),
    USES_TOT >= 10000
    ) %>%
  dplyr::group_by(LEMMA) %>%
  dplyr::mutate(CENT_DIFF = CENT_DEGREE - lag(CENT_DEGREE, default=CENT_DEGREE[1])) %>%
  filter(SUBSET == 'four') %>%
  select(-SUBSET) %>%
  rename(
    CENT_LAST = CENT_DEGREE,
    EDGES_LAST = EDGES
    ) %>%
  arrange((CENT_DIFF))
```


# Density

## overall

```{r}
df_comp %>%
  select(LEMMA, SUBSET, USES_TOT, CENT_DEGREE, DENSITY) %>%
  filter(SUBSET == 'four') %>%
  arrange(DENSITY)
```


## over time

### case studies

```{r}
dens_incr <- c('robo-signing')
dens_decr <- c('deep learning', 'artificial intelligence')

plt <- df_comp %>%
  select(LEMMA, SUBSETTING, SUBSET, CENT_DEGREE, DENSITY) %>%
  filter(
    SUBSET != 'full',
    LEMMA %in% c(cases, dens_decr)
  ) %>%
  ggplot(., aes(x=SUBSET, y=DENSITY)) + # group=1
    geom_point(aes(group=LEMMA, color=LEMMA, shape=LEMMA)) +
    geom_line(aes(group=LEMMA, color=LEMMA, linetype=LEMMA)) +
    guides(group=TRUE) +
    ggtitle('Diffusion over time: changes in degree centralization') +
    scale_y_continuous('density') +
    scale_x_discrete('subset')

ggplotly(plt)
# ggsave('out/cases_cent_diac.pdf', width=6, height=4)
```


### full sample

```{r}
df_comp %>%
  filter(
    SUBSET %in% c('one', 'two', 'three', 'four'),
    LEMMA %in% unsuccessful_diffs,
    USES_TOT >= 10000
    ) %>%
  group_by(SUBSET) %>%
  summarize(
    DENS_AVG = mean(DENSITY)
    ) %>%
  ggplot(., aes(x=SUBSET, y=DENS_AVG, group=1)) +
    geom_line() +
    geom_point() +
    scale_y_continuous('density') +
    scale_x_discrete('subsets')
```


### biggest changes

```{r}
df_comp %>%
  select(LEMMA, SUBSET, CENT_DEGREE, DENSITY, EDGES, USES_TOT) %>%
  filter(
    SUBSET %in% c(
      'one', 
      'four'
      ),
    USES_TOT >= 10000
    ) %>%
  dplyr::group_by(LEMMA) %>%
  dplyr::mutate(DENS_DIFF = DENSITY - lag(DENSITY, default=DENSITY[1])) %>%
  filter(SUBSET == 'four') %>%
  select(-SUBSET) %>%
  rename(
    DENS_LAST = DENSITY,
    EDGES_LAST = EDGES
    ) %>%
  arrange((DENS_DIFF))
```


# Frequency vs. networks

## Frequency vs. centralization

### Plot

#### Full sample

```{r}
df_comp %>%
  filter(
    SUBSET == 'four',
    # USES_TOT %in% (150000:500000)
    # LEMMA %in% c(cases)
    # !LEMMA %in% c('slut shaming', 'dashcam', 'shareable', 'cuckold', 'deep learning', 'hyperlocal')
    ) %>%
  select(LEMMA, CENT_DEGREE, USES_TOT, USES, EDGES) %>%
  ggplot(., aes(x=CENT_DEGREE, y=USES_TOT)) +
    geom_text(aes(label=LEMMA), hjust=-0.1, vjust=-0.1) + 
    # geom_point() +
    scale_y_continuous(
      'usage frequency (log)', 
      trans='log',
      labels=function(x)round(x,-3)
      ) +
    scale_x_continuous(
      'degree centralization (log)',
      trans='log',
      labels=function(x)round(x,3)
      )

# ggplotly(plt)
# ggsave('out/full_cent_freq_overall.pdf', width=6, height=4)
```


#### Case studies

```{r}
subset <- 'four'

df_cases <- df_comp %>%
  filter(
    SUBSET == subset,
    LEMMA %in% c(cases),
    !LEMMA %in% c(
      'upskill',
      'poppygate'
      )
    )

cases_freq_min <- df_cases %>%
  dplyr::summarise(USES_MIN = min(USES_TOT)) %>%
  pull(USES_MIN)
  
cases_freq_max <- df_cases %>%
  dplyr::summarise(USES_MAX = max(USES_TOT)) %>%
  mutate(USES_MAX = get_uses_tot_lemma(df_comp, 'birther')) %>%
  pull(USES_MAX)

df_comp %>%
  filter(
    SUBSET == subset,
    USES_TOT >= cases_freq_min,
    USES_TOT <= cases_freq_max,
    !LEMMA %in% c('big dick energy', 'slut shaming', 'cuckold', 'shareable', 'Brexiteer', 'incel', 'dashcam', 'refugee crisis')
    ) %>%
  select(LEMMA, CENT_DEGREE, USES_TOT, USES, EDGES) %>%
  ggplot(., aes(x=CENT_DEGREE, y=USES_TOT)) +
    geom_text(aes(label=LEMMA), color='black', hjust=.4, vjust=-.6) + 
    geom_text(data=df_cases, aes(label=LEMMA), color='blue', hjust=.4, vjust=-.6) +
    geom_point() + 
    scale_y_continuous(
      'usage frequency (log)', 
      trans='log',
      labels=function(x)round(x,-3)
      ) +
    scale_x_continuous(
      'degree centralization'
      # trans='log'
      )

# ggsave('out/cases_cent_freq_overall.pdf', width=6, height=4)
```


### Biggest discrepancies

```{r}
df_comp %>%
  filter(
    SUBSETTING == 'time',
    SUBSET == 'four'
    ) %>%
  select(LEMMA, USES_TOT, CENT_DEGREE) %>%
  mutate(DISC = USES_TOT / CENT_DEGREE) %>%
  arrange(DISC)
```


# Correlations

## FREQ X NETWORKS

```{r}
df_corr <- df_comp %>%
  filter(
    # SUBSET != 'full'
    # EDGES >= 100
    ) %>%
  select(-c(LEMMA, SUBSET, START, END, SKIP, STAMP))
  
cor.test(df_corr$USES, df_corr$CENT_DEGREE)
```

## COEF_VAR

```{r}
df_comp %>%
  # cor.test(df_corr$CENT_DEGREE, df_corr$COEF_VAR)
  # filter(SUBSET == 'full') %>%
  select(-c(LEMMA, SUBSET, START, END, SKIP, STAMP)) %>%
  summarize(
    # P = cor.test(COEF_VAR, CENT_DEGREE)$p.value,
    # EST = cor.test(COEF_VAR, CENT_DEGREE)$estimate
    P = cor.test(COEF_VAR, USES_TOT)$p.value,
    EST = cor.test(COEF_VAR, USES_TOT)$estimate
    # P = cor.test(CENT_DEGREE, USES)$p.value,
    # EST = cor.test(CENT_DEGREE, USES)$estimate
    )
```


# Coefficient of variation

```{r}
df_comp %>%
  filter(
    SUBSET == 'full',
    USES_TOT >= 1000
    ) %>%
  select(LEMMA, USES_TOT, COEF_VAR) %>%
  arrange(desc(COEF_VAR))
```


# Processing status

## Lemma list

```{r}
df_comp %>%
  select(LEMMA, SUBSET, STAMP) %>%
  filter(SUBSET == 'four') %>%
  # mutate(STAMP = as_datetime(STAMP)) %>%
  arrange(desc(STAMP))
```


## Dataset statistics

```{r}
df_comp %>%
  filter(SUBSET == 'full') %>%
  select(LEMMA, SUBSET, USES_TOT, USERS_TOT) %>%
  dplyr::summarise(
    USES_ALL = sum(USES_TOT),
    USERS_ALL = sum(USERS_TOT)
    )
```