---
author: 'Quirin WÃ¼rschinger'
title: "Social networks of lexical innovation"
output: 
  html_notebook: 
    toc: yes
---

# Load code

```{r include=FALSE}
source('src/load-data.R')
source('src/postproc.R')
source('src/uses.R')
source('src/users.R')
source('src/sna.R')

library(corrr)
library(tidyr)
```


# Variables

```{r include=FALSE}
subsetting = 'time'

cases <- c(
  'ghosting', 
  'lituation', 
  'alt-left', 
  'solopreneur'
  )

```


# Load data

```{r include=FALSE}
if (exists('df_comp') == FALSE) {
  df_comp <- read_df_comp()
}
```


# Filter data

```{r include=FALSE}
df_comp %<>% 
  filter(
    SKIP != TRUE,
    SUBSETTING == subsetting
    # successful words: USES_TOT >= 10000
    # unsuccessful words: USES_TOT <= 10000
    # weird words: big dick energy
    # established words from list
    )
```


# Single lemma

## Load data

```{r include=FALSE}
corpus <- '/Volumes/qjd/twint/'
lemma <- 'upskill'

tweets <- load_data(corpus, lemma)
tweets <- postproc(tweets)
tweets <- filter_tweets(tweets)
```


## Check tweets

```{r}
tweets %>%
  select(tweet, date) %>%
  # slice(., sample(1:n())) #random selection
  arrange(date)
```


# Case studies

## Data overview

```{r include=FALSE}
df_comp %>%
  filter(
    LEMMA %in% cases,
    SUBSET == 'full'
    ) %>%
  select(LEMMA, USES=USES_TOT, SPEAKERS=USERS_TOT) %>%
  mutate(USES_PER_SPEAKER = USES / SPEAKERS) %>%
  arrange(desc(USES))
  # ggplot(data=., aes(x=USERS_TOT, y=USES_TOT)) +
  #   geom_text(aes(label=LEMMA)) +
  #   scale_y_continuous('frequency (log)', trans='log')
```


## Usage frequency

### Overall across cases

```{r}
df_comp %>%
  filter(
    LEMMA %in% cases,
    SUBSET == 'full'
    ) %>%
   arrange(desc(USES_TOT))
```


### Uses in first subset

```{r}
df_comp %>%
  filter(
    SUBSETTING == 'time',
    SUBSET == 'one'
    ) %>%
  select(LEMMA, USES_TOT, USES) %>%
  arrange(USES)
```



## Degree centralization

### Diachronic

```{r}
plt <- df_comp %>%
  select(LEMMA, SUBSETTING, SUBSET, CENT_DEGREE, CENT_EV) %>%
  filter(
    SUBSET != 'full',
    LEMMA %in% cases
  ) %>%
  ggplot(., aes(x=SUBSET, y=CENT_DEGREE)) + # group=1
    geom_point(aes(group=LEMMA, color=LEMMA, shape=LEMMA)) +
    geom_line(aes(group=LEMMA, color=LEMMA, linetype=LEMMA)) +
    guides(group=FALSE) +
    ggtitle('Diffusion over time: changes in degree centralization') +
    scale_y_continuous('degree centrality') +
    scale_x_discrete('subset')

plt
ggsave('out/cases_cent_diac.pdf', width=6, height=4)
```


### Overall

```{r}
df_comp %>%
  filter(
    LEMMA %in% cases,
    SUBSET == 'full'
    ) %>%
  select(
    LEMMA, 
    EDGES,
    CENT_DEGREE, 
    CENT_EV
    ) %>%
  arrange((CENT_DEGREE))
```


# Full sample

## Usage intensity

```{r}
df_comp %>%
  filter(SUBSET == 'full') %>%
  arrange(desc(USES))
```


# Edges

## First subset

```{r include=FALSE}
df_comp %>%
  dplyr::group_by(LEMMA) %>%
  filter(SUBSET == 'one') %>%
  select(LEMMA, SUBSET, USES) %>%
  arrange(USES)
```


## Degree centrality

### Overall

#### List

```{r}
df_comp %>%
  select(LEMMA, SUBSET, USES, CENT_DEGREE, CENT_EV) %>%
  filter(
    SUBSET == 'full',
    USES >= 2
    ) %>%
  arrange(
    # (CENT_DEGREE)
    desc(CENT_EV)
  )
```


#### Plot

```{r}
plt <- df_comp %>%
  select(LEMMA, SUBSET, USES, CENT_DEGREE) %>%
  filter(SUBSET == 'full') %>%
  arrange((CENT_DEGREE)) %>%
  ggplot(., aes(x=CENT_DEGREE, y=reorder(LEMMA, CENT_DEGREE))) +
    geom_point() +
    scale_y_discrete('lemmas') +
    scale_x_continuous(
      'degree centralization (log)',
      trans='log'
      )

plt

# ggsave('out/cent_sync_all.pdf', width=6, height=4)
```


### Over time

#### Across all lemmas

```{r}
plt <- df_comp %>%
  filter(
    SUBSET %in% c('one', 'two', 'three', 'four'),
    USES_TOT >= 10000
    ) %>%
  group_by(SUBSET) %>%
  summarize(CENT_AVG = mean(CENT_DEGREE)) %>%
  ggplot(., aes(x=SUBSET, y=CENT_AVG, group=1)) +
    geom_line() +
    geom_point() +
    scale_y_continuous('degree centralization') +
    scale_x_discrete('subsets')

plt
ggsave('out/full_cent_diac.pdf', width=6, height=4)
```


#### Biggest changes

```{r}
df_comp %>%
  select(LEMMA, SUBSET, CENT_DEGREE, EDGES, USES_TOT) %>%
  filter(
    SUBSET %in% c(
      'one', 
      'four'
      ),
    USES_TOT >= 10000
    ) %>%
  dplyr::group_by(LEMMA) %>%
  dplyr::mutate(CENT_DIFF = CENT_DEGREE - lag(CENT_DEGREE, default=CENT_DEGREE[1])) %>%
  drop_na() %>%
  select(-SUBSET) %>%
  rename(
    CENT_LAST = CENT_DEGREE,
    EDGES_LAST = EDGES
    ) %>%
  arrange((CENT_DIFF))
```


# Frequency vs. networks

## Frequency vs. centralization

### Plot

```{r}
plt <- df_comp %>%
  filter(
    SUBSET == 'four',
    # USES_TOT %in% (150000:500000),
    # !LEMMA %in% c('slut shaming', 'dashcam', 'shareable', 'cuckold', 'deep learning', 'hyperlocal')
    ) %>%
  select(LEMMA, CENT_DEGREE, USES, USES, EDGES) %>%
  ggplot(., aes(x=CENT_DEGREE, y=USES)) +
    geom_text(aes(label=LEMMA), hjust=-0.1, vjust=-0.1) + 
    # geom_point() +
    scale_y_continuous(
      'usage frequency (log)', 
      trans='log'
      ) +
    scale_x_continuous(
      'degree centralization',
      # trans='log'
      )

ggplotly(plt)
ggsave('out/full_cent_freq_overall.pdf', width=6, height=4)
# ggsave('out/cases_cent_freq_overall.pdf', width=6, height=4)
```


### Biggest discrepancies

```{r}
df_comp %>%
  filter(
    SUBSETTING == 'time',
    SUBSET == 'full'
    ) %>%
  select(LEMMA, USES_TOT, SUBSET, USES, CENT_DEGREE) %>%
  mutate(DISC = USES_TOT / CENT_DEGREE) %>%
  arrange(DISC)
```


### Correlation

```{r}
df_corr <- df_comp %>%
  filter(
    # SUBSET != 'full'
    # EDGES >= 100
    ) %>%
  select(-c(LEMMA, SUBSET, START, END, SKIP, STAMP))
  
cor.test(df_corr$USES, df_corr$CENT_DEGREE)
```


# Coefficient of variation

```{r}
df_comp %>%
  filter(
    SUBSET == 'full',
    USES_TOT >= 1000
    ) %>%
  select(LEMMA, USES_TOT, COEF_VAR) %>%
  arrange(desc(COEF_VAR))
```


# Processing status

## Lemma list

```{r}
df_comp %>%
  select(LEMMA, SUBSET, STAMP) %>%
  filter(SUBSET == 'four') %>%
  # mutate(STAMP = as_datetime(STAMP)) %>%
  arrange(desc(STAMP))
```


## Dataset statistics

```{r}
df_comp %>%
  filter(SUBSET == 'full') %>%
  select(LEMMA, SUBSET, USES_TOT, USERS_TOT) %>%
  dplyr::summarise(
    USES_ALL = sum(USES_TOT),
    USERS_ALL = sum(USERS_TOT)
    )
```