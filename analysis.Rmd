---
author: 'Quirin WÃ¼rschinger'
title: "Social networks of lexical innovation"
output: 
  html_notebook: 
    toc: yes
---

# Load code

```{r include=FALSE}
source('src/load-data.R')
source('src/postproc.R')
source('src/uses.R')
source('src/users.R')
source('src/sna.R')

library(corrr)
library(tidyr)
```


# Load data

## single lemma

```{r include=FALSE}
corpus <- '/Volumes/qjd/twint/'
lemma <- 'Anglo-Saxon'

tweets <- load_data(corpus, lemma)
tweets <- postproc(tweets)
```

## metrics for all lemmas

```{r include=FALSE}
if (exists('df_comp') == FALSE) {
  df_comp <- read_df_comp()
}
```


# Filter data

```{r include=FALSE}
df_comp %<>% 
  filter(
    SKIP != TRUE,
    SUBSETTING == 'time'
    # successful words: USES_TOT >= 10000
    # unsuccessful words: USES_TOT <= 10000
    # weird words: big dick energy
    # established words from list
    )
```


## Uses in first subset

```{r}
df_comp %>%
  filter(
    SUBSETTING == 'time',
    SUBSET == 'one'
    ) %>%
  select(LEMMA, USES_TOT, USES) %>%
  arrange(USES)
```



# Case studies

```{r include=FALSE}
cases <- c(
  'ghosting', 
  'lituation', 
  'alt-left', 
  'solopreneur',
  'upcycling',
  'upskill',
  'baecation',
  'dashcam',
  'overtourism'
  )
```


## Check tweets

```{r}
tweets %>%
  select(tweet, date) %>%
  # slice(., sample(1:n())) #random selection
  arrange(date) %>%
  View()
```


## Usage frequency

### Overall across cases

```{r}
df_comp %>%
  filter(
    LEMMA %in% cases,
    SUBSET == 'full'
    ) %>%
   arrange(desc(USES_TOT))
```



### Single lemma

```{r}
uses <- get_uses(tweets)
uses_tot <- get_uses_tot(uses)
age = get_age(uses)
coef_var <- get_coef_var(uses)
mean_date <- get_mean_date(uses)
max_date <- get_max_date(uses)
uses_month <- conv_uses_month(uses)
uses_plt <- plt_uses(uses_month, lemma, mean_date, max_date)
ggplotly(uses_plt)
```


## Degree centralization

### Diachronic

```{r}
plt <- df_comp %>%
  select(LEMMA, SUBSETTING, SUBSET, CENT_DEGREE, CENT_EV) %>%
  filter(
    SUBSET != 'full',
    LEMMA %in% cases
  ) %>%
  ggplot(., aes(x=SUBSET, y=CENT_DEGREE)) + # group=1
    geom_point(aes(group=LEMMA, color=LEMMA, shape=LEMMA)) +
    geom_line(aes(group=LEMMA, color=LEMMA, linetype=LEMMA)) +
    guides(group=FALSE) +
    ggtitle('Diffusion over time: changes in degree centralization') +
    scale_y_continuous('degree centrality') +
    scale_x_discrete('subset')

plt

# ggsave('out/cent_deg_diac_Anglo-Saxon.pdf', width=6, height=4)
```


### Overall

```{r}
df_comp %>%
  filter(
    LEMMA %in% cases,
    SUBSET == 'full'
    ) %>%
  select(
    LEMMA, 
    EDGES,
    CENT_DEGREE, 
    CENT_EV
    ) %>%
  arrange((CENT_DEGREE))
```


# Comparative analyses


## Degree centrality


### Overall

#### List

```{r}
df_comp %>%
  select(LEMMA, SUBSET, USES, CENT_DEGREE, CENT_EV) %>%
  filter(
    SUBSET == 'full',
    USES >= 2
    ) %>%
  arrange(
    # (CENT_DEGREE)
    desc(CENT_EV)
  )
```


#### Plot

```{r}
plt <- df_comp %>%
  select(LEMMA, SUBSET, USES, CENT_DEGREE) %>%
  filter(SUBSET == 'full') %>%
  arrange((CENT_DEGREE)) %>%
  ggplot(., aes(x=CENT_DEGREE, y=reorder(LEMMA, CENT_DEGREE))) +
    geom_point() +
    scale_y_discrete('lemmas') +
    scale_x_continuous(
      'degree centralization (log)',
      trans='log'
      )

plt

# ggsave('out/cent_sync_all.pdf', width=6, height=4)
```


### Over time

#### Across all lemmas

```{r}
df_comp %>%
  filter(
    SUBSET %in% c('one', 'two', 'three', 'four'),
    USES_TOT >= 10000
    ) %>%
  group_by(SUBSET) %>%
  summarize(CENT_AVG = mean(CENT_DEGREE)) %>%
  ggplot(., aes(x=SUBSET, y=CENT_AVG, group=1)) +
    geom_line() +
    geom_point()
```


#### Biggest changes

```{r}
df_comp %>%
  select(LEMMA, SUBSET, CENT_DEGREE, EDGES, USES_TOT) %>%
  filter(
    SUBSET %in% c(
      'one', 
      'four'
      ),
    USES_TOT >= 10000
    ) %>%
  dplyr::group_by(LEMMA) %>%
  dplyr::mutate(CENT_DIFF = CENT_DEGREE - lag(CENT_DEGREE, default=CENT_DEGREE[1])) %>%
  drop_na() %>%
  select(-SUBSET) %>%
  rename(
    CENT_LAST = CENT_DEGREE,
    EDGES_LAST = EDGES
    ) %>%
  arrange((CENT_DIFF))
```


# Usage intensity

```{r}
df_comp %>%
  filter(SUBSET == 'full') %>%
  arrange(desc(USES))
```



# Usage intensity vs. network characteristics

## Uses vs. degree centralization

### Plot

```{r}
plt <- df_comp %>%
  filter(
    SUBSET == 'full'
    # USES %in% (10000:2000000),
    # USES %in% (150000:500000),
    # !LEMMA %in% c('slut shaming', 'dashcam', 'shareable', 'cuckold', 'deep learning', 'hyperlocal')
    ) %>%
  select(LEMMA, CENT_DEGREE, USES, EDGES) %>%
  ggplot(., aes(x=CENT_DEGREE, y=USES)) +
    geom_text(aes(label=LEMMA), hjust=-0.1, vjust=-0.1) + 
    geom_point() +
    scale_y_continuous(
      'usage frequency (log)', 
      trans='log'
      ) +
    scale_x_continuous(
      'degree centralization',
      # trans='log'
      ) +
    ggtitle(
      # 'Usage frequency vs. degree centralization: full sample'
      'Usage frequency vs. degree centralization: zooming on case study lexemes'
      )
    # geom_smooth(method=lm)

ggplotly(plt)
# ggsave('~/Desktop/freq-vs-net.pdf', width=6, height=4)
    
```


### Biggest discrepancies

```{r}
df_comp %>%
  filter(
    SUBSETTING == 'time',
    SUBSET == 'full'
    ) %>%
  select(LEMMA, USES_TOT, SUBSET, USES, CENT_DEGREE) %>%
  mutate(DISC = USES_TOT / CENT_DEGREE) %>%
  arrange(DISC)
```


### Correlation

```{r}
df_corr <- df_comp %>%
  filter(
    # SUBSET != 'full'
    # EDGES >= 100
    ) %>%
  select(-c(LEMMA, SUBSET, START, END, SKIP, STAMP))
  
cor.test(df_corr$USES, df_corr$CENT_DEGREE)
```


## Degree centrality vs. communities

### Correlation

```{r}
df_comp %>%
  filter(SUBSET == 'last') %>%
  select(CENT_DEGREE, COMMUNITIES) %>%
  mutate(COMMUNITIES = as.numeric(COMMUNITIES)) %>%
  correlate()
```


### Plot

```{r}
df_comp %>%
  filter(SUBSET == 'last') %>%
  select(LEMMA, CENT_DEGREE, COMMUNITIES) %>%
  ggplot(., aes(x=CENT_DEGREE, y=as.numeric(COMMUNITIES))) +
    geom_text(aes(label=LEMMA)) +
    scale_x_continuous(trans='log')
```



## Uses vs. users

### Plot

```{r}
plt <- df_comp %>%
  filter(SUBSET == 'full') %>%
  select(LEMMA, USES_TOT, USERS_TOT) %>%
  ggplot(., aes(x=USERS_TOT, y=USES_TOT)) +
    geom_text(aes(label=LEMMA)) +
    scale_x_continuous(trans='log') +
    scale_y_continuous(trans='log') +
    geom_smooth(method=lm)
ggplotly(plt)
```

### Correlation

```{r}
df_comp %>%
  filter(SUBSET == 'full') %>%
  select(USES_TOT, USERS_TOT) %>%
  correlate()
```


## Coefficient of variation

```{r}
df_comp %>%
  filter(
    SUBSET == 'full',
    USES_TOT >= 1000
    ) %>%
  select(LEMMA, USES_TOT, COEF_VAR) %>%
  arrange(desc(COEF_VAR))
```


## Processing status

### Lemma list

```{r}
df_comp %>%
  select(LEMMA, SUBSET, STAMP) %>%
  filter(SUBSET == 'four') %>%
  # mutate(STAMP = as_datetime(STAMP)) %>%
  arrange(desc(STAMP))
```


### Dataset statistics

```{r}
df_comp %>%
  filter(SUBSET == 'full') %>%
  select(LEMMA, SUBSET, USES_TOT, USERS_TOT) %>%
  dplyr::summarise(
    USES_ALL = sum(USES_TOT),
    USERS_ALL = sum(USERS_TOT)
    )
```


# COEF_VAR vs. CENT

```{r}
df_comp %>%
  filter(SUBSET == 'full') %>%
  select(LEMMA, COEF_VAR, CENT_DEGREE) %>%
  ggplot(., aes(y=COEF_VAR, x=CENT_DEGREE)) +
    geom_text(aes(label=LEMMA))
    # scale_y_continuous(trans='log')
```


# Correlations: EDA

```{r}
library(Hmisc)

df_corr <- df_comp %>%
  # filter(SUBSET == 'last') %>%
  # select(-c(LEMMA, SUBSETTING, SUBSET, START, END, SKIP, STAMP)) %>%
  # select(-c(USERS_TOT, AGE)) %>%
  # mutate(FOCUS = USES_TOT) %>%
  # focus(FOCUS) %>%
  # ggplot(., aes(reorder(rowname, FOCUS), FOCUS)) +
  # geom_col() +
  # coord_flip()
  # rearrange() %>%
  # shave() %>%
  # rplot()
  # network_plot(min_cor=.5) %>%

  
```


  